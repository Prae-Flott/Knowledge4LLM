# DelphiRAG: Enhancing LLMs Time Serial Inference with Human Experts Study

![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![Ollama](https://img.shields.io/badge/ollama-white?style=for-the-badge&logo=ollama&logoColor=black)

## Abstract

The rapid advancement of large language models (LLMs) has revealed their impressive capabilities in semantic understanding and logical reasoning over text-based data. These developments open new avenues for integrating LLM-based inference with high-value domain knowledge from human experts. However, expert knowledge is often unstructured and limited in volume, particularly within specific industrial domains, rendering traditional fine-tuning approaches impractical. In this work, we propose a novel and efficient framework that facilitates integrating expert knowledge from a Delphi study into LLMs. We validate our framework through an experiment on a predictive maintenance (PdM) use case involving mobile robotic systems, demonstrating notable inference performance improvements with human expertsâ€™ knowledge and the feasibility of leveraging expert knowledge in data-scarce environments.

## ğŸ“ Repository Structure

```text
Knowledge4LLM/
â”œâ”€â”€ doc/                # original knowledge from website and Expert study; and the questions for evaluation
â”œâ”€â”€ knowledge_base/     # embedded knowledge  
â”œâ”€â”€ utils/              # some sub-functions
â”‚   â”œâ”€â”€ data_to_text.py
â”‚   â”œâ”€â”€ embedding_utils.py
â”‚   â”œâ”€â”€ evaluation_utils_llm.py
â”‚   â””â”€â”€ evaluation_utils.py
â”œâ”€â”€ data_loader.py      # load the time serial data
â”œâ”€â”€ knowledge_loader.py # load the knowledges in /doc, and vectorized them
â”œâ”€â”€ evaluation.py       # evaluate the performance of the model with a statistical approaches
â”œâ”€â”€ evaluation_llm.py   # evaluate the performance another LLM
â””â”€â”€ main.py             # start a chat with the model
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10
- PyTorch
- Ollama
- Pandas, NumPy, Scikit-learn
- Jupyter 

### Run an example

- Run the knowledge-to-RAG transformation script:

```
python knowledge_loader.py
```

- Chat with the model by running:

```
python main.py
```

- evaluate the model performance with statistical approaches by running:

```
python evaluation.py
```

or if you want to evaluate the performance with another LLM

```
python evaluation_llm.py
```

add parameter `--data`, to switch the inference based on the time serial data.
